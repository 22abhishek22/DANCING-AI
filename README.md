# Dancing AI(Teaching an AI to Dance)
The project works on teaching an AI to create his own dance steps given an audio file as the input. I used **OpenCV and librosa** to work with the video and audio files and **keras LSTM** networks to train the AI.
 This project is inspired from the Siraj Raval's #InMyFeeling AI challenge. You can check the video [here](https://www.youtube.com/watch?v=prswDGGmYaE&feature=youtu.be).

The Dancing AI Project includes:-

 1. Pose detection in the given sample video using **OpenCV** and **COCO dataset** and storing coordinates generated by coordinates.ipynb in 'coordinates.csv'.(It took around 3 hours to process a one minute video with 30 fps on google colab.) Output video will look something like this_![.](https://github.com/keshavoct98/Dancing-AI/blob/master/output.gif)
 
 2. Converting Audio to floating point series in audio.ipynb file using **librosa** package and storing data in audio.csv
 
 3. Replacing missing values in corrdinates.csv file and combining 'coordinates.csv' and 'audio.csv' in final_df.ipynb into a single dataframe to store it in 'final_df.csv' file.
 
 4. Providing final dataframe as input to the **LSTM** network(pred.ipynb) using **audio time series as features** and **pose-detection coordinates as labels** and predicting new coordinates using audio time series test-set as input to LSTM model.
 
 5. Using predicted coordinates to generate dance video using video_generator.ipynb and writing generated frames in final video using **OpenCV VideoWriter**. *Take a look at my dancing AI*_![.](https://github.com/keshavoct98/Dancing-AI/blob/master/final.gif)
 
 ### Refrences:
 1. [OpenPoseCV](https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/)
 2. [Librosa](https://librosa.github.io/librosa/index.html)
